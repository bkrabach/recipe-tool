{
  "component_title": "Chat Assistant Panel",
  "purpose_statement": "The Chat Assistant Panel is a standalone UI component embedded in the sidebar that provides an interactive chat interface powered by an LLM endpoint. Users can enter prompts, view chat history, and have their inputs processed by the LLM, with responses displayed in real time. It also integrates with the Drawflow editor to add or update `llm_generate` nodes based on user interactions.",
  "core_requirements": [
    "Mount within the sidebar container element (e.g., `#sidebar`).",
    "Render a scrollable chat history area, text input field, and send button.",
    "Capture user prompts and send them via POST to a configurable LLM API endpoint.",
    "Display LLM responses in the chat history with timestamps or user/assistant labels.",
    "Indicate loading or pending state while awaiting LLM responses.",
    "Expose hooks to call `editor.addNode('llm_generate', config)` or `editor.updateNode(nodeId, config)` based on chat context.",
    "Gracefully handle network or response errors and display user-friendly error messages.",
    "Allow programmatic clearing or resetting of the chat history.",
    "Clean up event listeners and DOM nodes on teardown."
  ],
  "implementation_considerations": [
    "Implement as a plain ES module (`chat_assistant_panel.js`) with no framework dependencies.",
    "Export an initialization function accepting a container element and a Drawflow editor instance.",
    "Use the Fetch API for network requests; avoid additional HTTP libraries.",
    "Maintain only minimal internal state (e.g., message list) and update the DOM directly.",
    "Use scoped CSS classes to style chat UI elements, relying on existing styles for consistency.",
    "Render messages by creating and appending DOM nodes; avoid virtual DOM abstractions.",
    "Debounce or disable the send button to prevent duplicate requests.",
    "Read the LLM endpoint URL and API key from a configuration object or environment variable.",
    "Ensure proper cleanup of timers and event listeners when the panel is removed."
  ],
  "component_dependencies": {
    "internal_components": [
      "Drawflow editor instance (from `core/canvas.js`)",
      "Toolbox Shell UI (`ui/toolbox_shell.js`) for layout consistency"
    ],
    "external_libraries": [],
    "configuration_dependencies": [
      "LLM_API_ENDPOINT",
      "LLM_API_KEY (if required by the endpoint)"
    ]
  },
  "output_files": [
    {
      "path": "src/ui/chat_assistant_panel.js",
      "description": "ES module implementing the chat assistant panel UI and its integration with the Drawflow editor and LLM endpoint"
    }
  ],
  "logging_requirements": {
    "debug": [
      "ChatAssistantPanel initialized with container and editor",
      "Sending prompt to LLM endpoint: {prompt}",
      "Received raw response from LLM: {response}",
      "Invoking editor API: {action}",
      "Network request completed in {duration}ms"
    ],
    "info": [
      "User message submitted",
      "LLM response rendered in chat history"
    ]
  },
  "error_handling": [
    {
      "error_type": "NetworkError",
      "error_message": "Failed to fetch LLM response",
      "recovery_action": "Display an inline error message in the chat UI and re-enable the send button for retry"
    },
    {
      "error_type": "InvalidResponseError",
      "error_message": "Unexpected response format from LLM",
      "recovery_action": "Log the raw response to console.debug and show a generic error message to the user"
    }
  ],
  "dependency_integration_considerations": [
    "Ensure the configured LLM endpoint supports CORS for browser requests",
    "Pass the Drawflow `editor` instance when initializing to allow node manipulation",
    "Align styling with the existing sidebar layout and CSS variables",
    "Document any required HTML container structure (e.g., `#sidebar`)",
    "Allow future extension for streaming responses or alternative endpoints"
  ]
}