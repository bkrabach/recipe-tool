# This file was generated by Codebase-Generator, do not edit directly
import os
import time
import logging
from typing import Optional, List, Type, Union, Dict, Any

from pydantic import BaseModel
from pydantic_ai import Agent
from pydantic_ai.settings import ModelSettings
from pydantic_ai.models.openai import OpenAIModel, OpenAIResponsesModelSettings
from pydantic_ai.providers.openai import OpenAIProvider
from pydantic_ai.models.anthropic import AnthropicModel
from pydantic_ai.mcp import MCPServer

from recipe_executor.llm_utils.azure_openai import get_azure_openai_model
from recipe_executor.llm_utils.responses import create_openai_responses_model
from recipe_executor.llm_utils.azure_responses import create_azure_responses_model

# Built-in tool parameter types for Responses API
from openai.types.responses import WebSearchToolParam, FileSearchToolParam


def get_model(model_id: str, logger: logging.Logger) -> Any:
    """
    Initialize an LLM model based on a standardized model_id string.
    Expected format: 'provider/model_name' or 'provider/model_name/deployment_name'.
    Supported providers:
      - openai
      - azure
      - anthropic
      - ollama
      - openai_responses
      - azure_responses
    """
    parts = model_id.split("/")
    if len(parts) < 2:
        raise ValueError(f"Invalid model_id format: '{model_id}'")
    provider = parts[0].lower()

    # OpenAI provider
    if provider == "openai":
        if len(parts) != 2:
            raise ValueError(f"Invalid OpenAI model_id: '{model_id}'")
        model_name = parts[1]
        return OpenAIModel(model_name)

    # Azure OpenAI provider
    if provider == "azure":
        if len(parts) == 2:
            model_name = parts[1]
            deployment = None
        elif len(parts) == 3:
            model_name, deployment = parts[1], parts[2]
        else:
            raise ValueError(f"Invalid Azure model_id: '{model_id}'")
        return get_azure_openai_model(
            logger=logger,
            model_name=model_name,
            deployment_name=deployment,
        )

    # Anthropic provider
    if provider == "anthropic":
        if len(parts) != 2:
            raise ValueError(f"Invalid Anthropic model_id: '{model_id}'")
        return AnthropicModel(parts[1])

    # Ollama provider
    if provider == "ollama":
        if len(parts) != 2:
            raise ValueError(f"Invalid Ollama model_id: '{model_id}'")
        base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
        provider_obj = OpenAIProvider(base_url=f"{base_url}/v1")
        return OpenAIModel(model_name=parts[1], provider=provider_obj)

    # OpenAI Responses API
    if provider == "openai_responses":
        if len(parts) != 2:
            raise ValueError(f"Invalid OpenAI Responses model_id: '{model_id}'")
        return create_openai_responses_model(parts[1])

    # Azure Responses API
    if provider == "azure_responses":
        if len(parts) == 2:
            model_name = parts[1]
            deployment = None
        elif len(parts) == 3:
            model_name, deployment = parts[1], parts[2]
        else:
            raise ValueError(f"Invalid Azure Responses model_id: '{model_id}'")
        return create_azure_responses_model(model_name, deployment)

    raise ValueError(f"Unsupported LLM provider: '{provider}' in model_id '{model_id}'")


class LLM:
    """
    Unified interface for interacting with various LLM providers
    and optional MCP servers.
    """

    def __init__(
        self,
        logger: logging.Logger,
        model: str = "openai/gpt-4o",
        max_tokens: Optional[int] = None,
        mcp_servers: Optional[List[MCPServer]] = None,
    ):
        self.logger: logging.Logger = logger
        self.default_model_id: str = model
        self.default_max_tokens: Optional[int] = max_tokens
        self.default_mcp_servers: List[MCPServer] = mcp_servers or []

    async def generate(
        self,
        prompt: str,
        model: Optional[str] = None,
        max_tokens: Optional[int] = None,
        output_type: Type[Union[str, BaseModel]] = str,
        mcp_servers: Optional[List[MCPServer]] = None,
        openai_builtin_tools: Optional[List[Dict[str, Any]]] = None,
    ) -> Union[str, BaseModel]:
        """
        Generate an output from the LLM based on the provided prompt.
        """
        model_id = model or self.default_model_id
        tokens = max_tokens if max_tokens is not None else self.default_max_tokens
        servers = mcp_servers if mcp_servers is not None else self.default_mcp_servers

        # Info: selected provider and model
        try:
            provider = model_id.split("/", 1)[0]
        except Exception:
            provider = "unknown"
        self.logger.info(
            "LLM generate using provider=%s model_id=%s",
            provider,
            model_id,
        )

        # Debug: request payload (masking sensitive info)
        output_name = getattr(output_type, "__name__", str(output_type))
        self.logger.debug(
            "LLM request payload prompt=%r model_id=%s max_tokens=%s output_type=%s openai_builtin_tools=%s",
            prompt,
            model_id,
            tokens,
            output_name,
            openai_builtin_tools,
        )

        # Initialize model instance
        try:
            model_instance = get_model(model_id, self.logger)
        except ValueError as e:
            self.logger.error("Invalid model_id '%s': %s", model_id, str(e))
            raise

        # Prepare model settings
        model_settings: Optional[Any] = None
        if provider == "openai_responses":
            settings_kwargs: Dict[str, Any] = {}
            if tokens is not None:
                settings_kwargs["max_tokens"] = tokens
            if openai_builtin_tools:
                converted: List[Union[WebSearchToolParam, FileSearchToolParam]] = []
                for t in openai_builtin_tools:
                    try:
                        converted.append(WebSearchToolParam(**t))
                    except Exception:
                        try:
                            converted.append(FileSearchToolParam(**t))
                        except Exception:
                            raise ValueError(f"Unsupported tool param: {t}")
                settings_kwargs["openai_builtin_tools"] = converted
            if settings_kwargs:
                model_settings = OpenAIResponsesModelSettings(**settings_kwargs)
        else:
            if tokens is not None:
                model_settings = ModelSettings(max_tokens=tokens)

        # Build Agent
        agent_kwargs: Dict[str, Any] = {
            "model": model_instance,
            "output_type": output_type,
            "mcp_servers": servers,
        }
        if model_settings is not None:
            agent_kwargs["model_settings"] = model_settings
        agent: Agent = Agent(**agent_kwargs)  # type: ignore

        # Execute the request
        start = time.time()
        try:
            async with agent.run_mcp_servers():
                result = await agent.run(prompt)
        except Exception as e:
            self.logger.error("LLM call failed for model_id=%s error=%s", model_id, str(e))
            raise
        end = time.time()

        # Log usage and timing
        duration = end - start
        usage = None
        try:
            usage = result.usage()
        except Exception:
            usage = None
        if usage:
            self.logger.info(
                "LLM result time=%.3f sec requests=%d tokens_total=%d (req=%d res=%d)",
                duration,
                usage.requests,
                usage.total_tokens,
                usage.request_tokens,
                usage.response_tokens,
            )
        else:
            self.logger.info("LLM result time=%.3f sec (usage unavailable)", duration)

        # Debug: raw result payload
        self.logger.debug("LLM raw result: %r", result)

        return result.output
