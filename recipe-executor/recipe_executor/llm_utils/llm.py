# This file was generated by Codebase-Generator, do not edit directly
import logging
import os
import time
from typing import Optional, List, Type, Union

from pydantic import BaseModel
from pydantic_ai import Agent
from pydantic_ai.models.openai import OpenAIModel
from pydantic_ai.models.anthropic import AnthropicModel
from pydantic_ai.providers.openai import OpenAIProvider
from pydantic_ai.settings import ModelSettings

from recipe_executor.llm_utils.azure_openai import get_azure_openai_model
from recipe_executor.llm_utils.responses import create_openai_responses_model
from recipe_executor.llm_utils.mcp import MCPServer


class LLM:
    """
    LLM provides a unified interface for interacting with various large language model providers
    and optional MCP servers. It handles model initialization, request formatting, and result
    processing, enabling the Recipe Executor to generate content and orchestrate external tools
    through a single API.
    """

    def __init__(
        self,
        logger: logging.Logger,
        model: str = "openai/gpt-4o",
        max_tokens: Optional[int] = None,
        mcp_servers: Optional[List[MCPServer]] = None,
    ):
        """
        Initialize the LLM component.

        Args:
            logger (logging.Logger): Logger for logging messages.
            model (str): Model identifier in the format 'provider/model_name' (or
                'provider/model_name/deployment_name').
            max_tokens (int): Maximum number of tokens for the LLM response.
            mcp_servers (Optional[List[MCPServer]]): List of MCP servers for access to tools.
        """
        self.logger = logger
        self.model = model
        self.max_tokens = max_tokens
        self.mcp_servers: List[MCPServer] = mcp_servers or []

    def _get_model(self, model_id: str) -> Union[OpenAIModel, AnthropicModel]:  # noqa: C901
        """
        Initialize an LLM model based on a standardized model_id string.

        Expected format: 'provider/model_name' or 'provider/model_name/deployment_name'.
        Supported providers:
        - openai
        - azure
        - anthropic
        - ollama
        - openai_responses
        """
        parts = model_id.split("/")
        provider = parts[0]

        if provider == "openai":
            if len(parts) != 2:
                raise ValueError(f"Invalid OpenAI model identifier: {model_id}")
            model_name = parts[1]
            return OpenAIModel(model_name)

        if provider == "azure":
            # azure/model_name[/deployment_name]
            if len(parts) == 2:
                model_name, deployment = parts[1], None
            elif len(parts) == 3:
                model_name, deployment = parts[1], parts[2]
            else:
                raise ValueError(f"Invalid Azure model identifier: {model_id}")
            return get_azure_openai_model(
                logger=self.logger, model_name=model_name, deployment_name=deployment
            )

        if provider == "anthropic":
            if len(parts) != 2:
                raise ValueError(f"Invalid Anthropic model identifier: {model_id}")
            model_name = parts[1]
            return AnthropicModel(model_name)

        if provider == "ollama":
            if len(parts) != 2:
                raise ValueError(f"Invalid Ollama model identifier: {model_id}")
            model_name = parts[1]
            base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
            return OpenAIModel(
                model_name=model_name,
                provider=OpenAIProvider(base_url=f"{base_url}/v1"),
            )

        if provider == "openai_responses":
            if len(parts) != 2:
                raise ValueError(f"Invalid OpenAI Responses model identifier: {model_id}")
            model_name = parts[1]
            return create_openai_responses_model(model_name)

        raise ValueError(f"Unsupported provider: {provider}")

    async def generate(
        self,
        prompt: str,
        model: Optional[str] = None,
        max_tokens: Optional[int] = None,
        output_type: Type[Union[str, BaseModel]] = str,
        mcp_servers: Optional[List[MCPServer]] = None,
    ) -> Union[str, BaseModel]:
        """
        Generate an output from the LLM based on the provided prompt.

        Args:
            prompt (str): The prompt string to be sent to the LLM.
            model (Optional[str]): Model identifier in the format 'provider/model_name'
                (or 'provider/model_name/deployment_name'). If not provided, uses
                the default set at initialization.
            max_tokens (Optional[int]): Maximum number of tokens for the LLM response.
                If not provided, uses the default set at initialization.
            output_type (Type[Union[str, BaseModel]]): The requested type for the LLM output.
                - str: Plain text output.
                - BaseModel: Structured output based on the provided Pydantic model.
            mcp_servers (Optional[List[MCPServer]]): List of MCP servers for tool integration.
                If not provided, uses the default set at initialization.

        Returns:
            Union[str, BaseModel]: The output from the LLM.

        Raises:
            ValueError: If model identifier is invalid or provider unsupported.
            Exception: For network, API, or MCP errors.
        """
        model_id = model or self.model
        max_toks = max_tokens if max_tokens is not None else self.max_tokens
        servers = mcp_servers if mcp_servers is not None else self.mcp_servers

        # Initialize model and agent
        llm_model = self._get_model(model_id)
        agent = Agent(model=llm_model, output_type=output_type, mcp_servers=servers)

        # Build model settings
        settings = None
        if max_toks is not None:
            settings = ModelSettings(max_tokens=max_toks)

        # Logging before the call
        provider, _, name = model_id.partition("/")
        self.logger.info(f"Calling LLM provider={provider}, model={name}")
        self.logger.debug(
            f"Request payload: prompt={prompt!r}, model_settings={settings!r}, mcp_servers={servers!r}"
        )

        start = time.perf_counter()
        try:
            if servers:
                async with agent.run_mcp_servers():
                    result = await agent.run(prompt, model_settings=settings)
            else:
                result = await agent.run(prompt, model_settings=settings)
        except Exception as e:
            self.logger.error(f"LLM call failed: {e}")
            raise

        duration = time.perf_counter() - start
        # Attempt to extract usage
        try:
            usage = result.usage()
            total_tokens = usage.total_tokens
        except Exception:
            total_tokens = None

        # Logging after the call
        if total_tokens is not None:
            self.logger.info(
                f"LLM call completed in {duration:.2f}s, tokens used: {total_tokens}"
            )
        else:
            self.logger.info(f"LLM call completed in {duration:.2f}s")

        self.logger.debug(f"Full result payload: {result!r}")
        return result.output
