# This file was generated by Codebase-Generator, do not edit directly
import os
import time
import logging
from typing import Optional, List, Type, Union, Any, Dict

from pydantic import BaseModel
from pydantic_ai import Agent
from pydantic_ai.settings import ModelSettings
from pydantic_ai.models.openai import OpenAIModel, OpenAIResponsesModelSettings
from pydantic_ai.providers.openai import OpenAIProvider
from pydantic_ai.models.anthropic import AnthropicModel
from pydantic_ai.mcp import MCPServer

from recipe_executor.llm_utils.azure_openai import get_azure_openai_model
from recipe_executor.llm_utils.responses import get_openai_responses_model
from recipe_executor.llm_utils.azure_responses import create_azure_responses_model

from openai.types.responses import WebSearchToolParam, FileSearchToolParam


def get_model(model_id: str, logger: logging.Logger) -> Any:
    """
    Initialize an LLM model based on a standardized model_id string.
    Expected format: 'provider/model_name' or 'provider/model_name/deployment_name'.

    Supported providers:
    - openai
    - azure
    - anthropic
    - ollama
    - openai_responses
    - azure_responses

    Args:
        model_id (str): Model identifier.
        logger (logging.Logger): Logger for diagnostic messages.

    Returns:
        A PydanticAI model instance for the specified provider and model.

    Raises:
        ValueError: If model_id format is invalid or provider unsupported.
    """
    parts = model_id.split("/")
    if len(parts) < 2:
        raise ValueError(f"Invalid model_id format: '{model_id}'")
    provider = parts[0].lower()

    # OpenAI provider
    if provider == "openai":
        if len(parts) != 2:
            raise ValueError(f"Invalid OpenAI model_id: '{model_id}'")
        model_name = parts[1]
        return OpenAIModel(model_name)

    # Azure OpenAI provider
    if provider == "azure":
        if len(parts) == 2:
            model_name = parts[1]
            deployment_name = None
        elif len(parts) == 3:
            model_name, deployment_name = parts[1], parts[2]
        else:
            raise ValueError(f"Invalid Azure model_id: '{model_id}'")
        return get_azure_openai_model(logger=logger, model_name=model_name, deployment_name=deployment_name)

    # Anthropic provider
    if provider == "anthropic":
        if len(parts) != 2:
            raise ValueError(f"Invalid Anthropic model_id: '{model_id}'")
        return AnthropicModel(parts[1])

    # Ollama provider (uses OpenAIModel with custom provider URL)
    if provider == "ollama":
        if len(parts) != 2:
            raise ValueError(f"Invalid Ollama model_id: '{model_id}'")
        base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
        provider_obj = OpenAIProvider(base_url=f"{base_url}/v1")
        return OpenAIModel(model_name=parts[1], provider=provider_obj)

    # OpenAI Responses API provider
    if provider == "openai_responses":
        if len(parts) != 2:
            raise ValueError(f"Invalid OpenAI Responses model_id: '{model_id}'")
        return get_openai_responses_model(logger=logger, model_name=parts[1])

    # Azure Responses API provider
    if provider == "azure_responses":
        if len(parts) == 2:
            model_name, deployment_name = parts[1], None
        elif len(parts) == 3:
            model_name, deployment_name = parts[1], parts[2]
        else:
            raise ValueError(f"Invalid Azure Responses model_id: '{model_id}'")
        return create_azure_responses_model(logger=logger, model_name=model_name, deployment_name=deployment_name)

    raise ValueError(f"Unsupported LLM provider: '{provider}' in model_id '{model_id}'")


class LLM:
    """
    Unified interface for interacting with various LLM providers
    and optional MCP servers.
    """

    def __init__(
        self,
        logger: logging.Logger,
        model: str = "openai/gpt-4o",
        max_tokens: Optional[int] = None,
        mcp_servers: Optional[List[MCPServer]] = None,
    ):
        self.logger: logging.Logger = logger
        self.default_model_id: str = model
        self.default_max_tokens: Optional[int] = max_tokens
        self.default_mcp_servers: List[MCPServer] = mcp_servers or []

    async def generate(
        self,
        prompt: str,
        model: Optional[str] = None,
        max_tokens: Optional[int] = None,
        output_type: Type[Union[str, BaseModel]] = str,
        mcp_servers: Optional[List[MCPServer]] = None,
        openai_builtin_tools: Optional[List[Dict[str, Any]]] = None,
    ) -> Union[str, BaseModel]:
        """
        Generate an output from the LLM based on the provided prompt.
        """
        model_id = model or self.default_model_id
        tokens = max_tokens if max_tokens is not None else self.default_max_tokens
        servers = mcp_servers if mcp_servers is not None else self.default_mcp_servers

        # Info log: model selection
        try:
            provider = model_id.split("/", 1)[0]
        except Exception:
            provider = "unknown"
        self.logger.info("LLM generate using provider=%s model_id=%s", provider, model_id)

        # Debug log: request details
        output_name = getattr(output_type, "__name__", str(output_type))
        self.logger.debug(
            "LLM request payload prompt=%r model_id=%s max_tokens=%s output_type=%s "
            "openai_builtin_tools=%r mcp_servers=%r",
            prompt,
            model_id,
            tokens,
            output_name,
            openai_builtin_tools,
            servers,
        )

        # Initialize model instance
        try:
            model_instance = get_model(model_id, self.logger)
        except ValueError as e:
            self.logger.error("Invalid model_id '%s': %s", model_id, str(e))
            raise

        # Prepare agent arguments
        agent_kwargs: Dict[str, Any] = {
            "model": model_instance,
            "output_type": output_type,
            "mcp_servers": servers,
        }

        # Configure model settings and built-in tools for Responses API
        base_provider = provider.lower()
        if base_provider in ("openai_responses", "azure_responses") and openai_builtin_tools:
            tool_params: List[Union[WebSearchToolParam, FileSearchToolParam]] = []
            for tool in openai_builtin_tools:
                ttype = tool.get("type")
                if hasattr(WebSearchToolParam, "__annotations__") and ttype in WebSearchToolParam.__annotations__:
                    tool_params.append(WebSearchToolParam(**tool))  # type: ignore
                else:
                    tool_params.append(FileSearchToolParam(**tool))  # type: ignore
            # Include max_tokens if provided
            if tokens is not None:
                settings = OpenAIResponsesModelSettings(
                    openai_builtin_tools=tool_params,
                    max_tokens=tokens,
                )
            else:
                settings = OpenAIResponsesModelSettings(openai_builtin_tools=tool_params)
            agent_kwargs["model_settings"] = settings
        elif tokens is not None:
            agent_kwargs["model_settings"] = ModelSettings(max_tokens=tokens)

        agent: Agent = Agent(**agent_kwargs)  # type: ignore

        # Execute request
        start_time = time.time()
        try:
            async with agent.run_mcp_servers():
                result = await agent.run(prompt)
        except Exception as e:
            self.logger.error("LLM call failed for model_id=%s error=%s", model_id, str(e))
            raise
        end_time = time.time()

        # Log result summary
        try:
            usage = result.usage()
        except Exception:
            usage = None
        duration = end_time - start_time
        if usage:
            self.logger.info(
                "LLM result time=%.3f sec requests=%d tokens_total=%d (req=%d res=%d)",
                duration,
                usage.requests,
                usage.total_tokens,
                usage.request_tokens,
                usage.response_tokens,
            )
        else:
            self.logger.info("LLM result time=%.3f sec (usage unavailable)", duration)

        # Debug log: raw result
        self.logger.debug("LLM raw result: %r", result)

        return result.output
