# This file was generated by Codebase-Generator, do not edit directly
import os
import time
import logging
from typing import Optional, List, Type, Union

from pydantic import BaseModel
from pydantic_ai import Agent
from pydantic_ai.models.openai import OpenAIModel
from pydantic_ai.providers.openai import OpenAIProvider
from pydantic_ai.models.anthropic import AnthropicModel
from pydantic_ai.mcp import MCPServer

from recipe_executor.llm_utils.azure_openai import get_azure_openai_model
from recipe_executor.llm_utils.responses import create_openai_responses_model
from recipe_executor.llm_utils.azure_responses import create_azure_responses_model


DEFAULT_MODEL = os.getenv("DEFAULT_MODEL", "openai/gpt-4o")
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")


class LLM:
    """
    LLM component provides unified interface for interacting with various LLM providers
    and optional MCP servers.
    """

    def __init__(
        self,
        logger: logging.Logger,
        model: str = DEFAULT_MODEL,
        max_tokens: Optional[int] = None,
        mcp_servers: Optional[List[MCPServer]] = None,
    ):
        """
        Initialize the LLM component.
        Args:
            logger (logging.Logger): Logger for logging messages.
            model (str): Model identifier in the format 'provider/model_name' or 'provider/model_name/deployment_name'.
            max_tokens (int): Maximum number of tokens for the LLM response.
            mcp_servers (Optional[List[MCPServer]]): List of MCP servers for access to tools.
        """
        self.logger = logger
        self.model = model
        self.max_tokens = max_tokens
        self.mcp_servers = mcp_servers or []

    async def generate(
        self,
        prompt: str,
        model: Optional[str] = None,
        max_tokens: Optional[int] = None,
        output_type: Type[Union[str, BaseModel]] = str,
        mcp_servers: Optional[List[MCPServer]] = None,
    ) -> Union[str, BaseModel]:
        """
        Generate an output from the LLM based on the provided prompt.

        Args:
            prompt (str): The prompt string to be sent to the LLM.
            model (Optional[str]): Model identifier in the format 'provider/model_name' or 'provider/model_name/deployment_name'.
            max_tokens (Optional[int]): Maximum number of tokens for the LLM response.
            output_type (Type[Union[str, BaseModel]]): The requested type for the LLM output.
            mcp_servers (Optional[List[MCPServer]]): List of MCP servers for access to tools.

        Returns:
            Union[str, BaseModel]: The output from the LLM.

        Raises:
            ValueError: If model ID is invalid or provider unsupported.
            Exception: For API or network errors.
        """
        # Determine effective parameters
        model_id = model or self.model
        max_toks = max_tokens if max_tokens is not None else self.max_tokens
        servers = mcp_servers if mcp_servers is not None else self.mcp_servers

        # Initialize model
        try:
            llm_model = get_model(model_id)
        except Exception as e:
            self.logger.error(f"Failed to initialize model '{model_id}': {e}")
            raise

        # Create agent
        agent_kwargs = {
            "model": llm_model,
            "output_type": output_type,
            "mcp_servers": servers,
        }
        if max_toks is not None:
            agent_kwargs["model_settings"] = {"max_tokens": max_toks}
        agent = Agent(**agent_kwargs)

        # Logging
        provider, model_name, *_ = model_id.split("/", 2)
        self.logger.info(f"Calling LLM provider={provider} model={model_name}")
        self.logger.debug(
            f"Request payload: prompt={prompt!r}, max_tokens={max_toks}, mcp_servers={len(servers)}"
        )

        # Call agent
        start = time.time()
        try:
            if servers:
                async with agent.run_mcp_servers():  # type: ignore
                    result = await agent.run(prompt)
            else:
                result = await agent.run(prompt)
        except Exception as e:
            self.logger.error(f"LLM call failed: {e}")
            raise
        duration = time.time() - start

        # Log result details
        usage = result.usage()
        self.logger.debug(f"LLM result payload: {result}")
        self.logger.info(
            f"LLM call completed in {duration:.2f}s | requests={usage.requests} "
            f"request_tokens={usage.request_tokens} response_tokens={usage.response_tokens} "
            f"total_tokens={usage.total_tokens}"
        )

        return result.output


def get_model(model_id: str) -> Union[OpenAIModel, AnthropicModel]:
    """
    Initialize an LLM model based on a standardized model_id string.
    Expected format: 'provider/model_name' or 'provider/model_name/deployment_name'.
    """
    if not model_id or "/" not in model_id:
        raise ValueError(f"Invalid model identifier '{model_id}'")
    parts = model_id.split("/")
    provider = parts[0]
    model_name = parts[1]
    deployment = parts[2] if len(parts) > 2 else None

    if provider == "openai":
        return OpenAIModel(model_name)
    if provider == "azure":
        # Azure OpenAI
        return get_azure_openai_model(logger=logging.getLogger(), model_name=model_name, deployment_name=deployment)
    if provider == "anthropic":
        return AnthropicModel(model_name)
    if provider == "ollama":
        # Ollama via OpenAI API compatibility
        return OpenAIModel(model_name, provider=OpenAIProvider(base_url=f"{OLLAMA_BASE_URL}/v1"))
    if provider == "openai_responses":
        return create_openai_responses_model(model_name)
    if provider == "azure_responses":
        return create_azure_responses_model(model_name, deployment)
    raise ValueError(f"Unsupported provider '{provider}' in model identifier")
