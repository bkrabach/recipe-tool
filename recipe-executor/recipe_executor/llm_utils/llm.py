# This file was generated by Codebase-Generator, do not edit directly
import os
import logging
from typing import Optional, List, Dict, Any, Type, Union

from pydantic import BaseModel
from pydantic_ai import Agent
from pydantic_ai.mcp import MCPServer

from recipe_executor.llm_utils.azure_openai import get_azure_openai_model
from recipe_executor.llm_utils.responses import create_openai_responses_model
from recipe_executor.llm_utils.azure_responses import create_azure_responses_model

# For Ollama usage
from pydantic_ai.models.openai import OpenAIModel
from pydantic_ai.providers.openai import OpenAIProvider
# Responses API settings
from pydantic_ai.models.openai import OpenAIResponsesModelSettings


def get_model(
    model_id: str,
    logger: logging.Logger,
) -> Any:
    """
    Initialize an LLM model based on a standardized model_id string.
    Expected format: 'provider/model_name' or 'provider/model_name/deployment_name'.
    """
    parts = model_id.split('/')
    if not parts or len(parts) < 2:
        raise ValueError(f"Invalid model identifier '{model_id}', expected 'provider/model_name'.")
    provider = parts[0]
    model_name = parts[1]

    # Azure OpenAI
    if provider == 'azure':
        # use Azure-specific initializer
        deployment = parts[2] if len(parts) >= 3 else None
        return get_azure_openai_model(
            logger=logger,
            model_name=model_name,
            deployment_name=deployment,
        )
    # OpenAI
    if provider == 'openai':
        return OpenAIModel(model_name)
    # Anthropic
    if provider == 'anthropic':
        from pydantic_ai.models.anthropic import AnthropicModel

        return AnthropicModel(model_name)
    # Ollama: use OpenAIModel with custom base_url
    if provider == 'ollama':
        base = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')
        provider_obj = OpenAIProvider(base_url=f"{base}/v1")
        return OpenAIModel(model_name=model_name, provider=provider_obj)
    # OpenAI Responses API
    if provider == 'openai_responses':
        return create_openai_responses_model(model_name)
    # Azure Responses API
    if provider == 'azure_responses':
        deployment = parts[2] if len(parts) >= 3 else None
        return create_azure_responses_model(model_name, deployment)

    raise ValueError(f"Unsupported model provider '{provider}'.")


class LLM:
    """
    LLM component providing a unified interface to multiple LLM providers.
    """

    def __init__(
        self,
        logger: logging.Logger,
        model: str = "openai/gpt-4o",
        max_tokens: Optional[int] = None,
        mcp_servers: Optional[List[MCPServer]] = None,
    ):
        self.logger = logger
        self.model = model
        self.max_tokens = max_tokens
        self.mcp_servers = mcp_servers or []

    async def generate(
        self,
        prompt: str,
        model: Optional[str] = None,
        max_tokens: Optional[int] = None,
        output_type: Type[Union[str, BaseModel]] = str,
        mcp_servers: Optional[List[MCPServer]] = None,
        openai_builtin_tools: Optional[List[Dict[str, Any]]] = None,
    ) -> Union[str, BaseModel]:
        """
        Generate an output from the LLM based on the provided prompt.
        """
        # Resolve settings
        model_id = model or self.model
        tokens = max_tokens if max_tokens is not None else self.max_tokens
        servers = mcp_servers if mcp_servers is not None else self.mcp_servers or []

        # Parse provider for logging
        parts = model_id.split('/')
        provider = parts[0] if parts else ''
        name = parts[1] if len(parts) > 1 else ''

        # Initialize model
        try:
            llm_model = get_model(model_id, self.logger)
        except Exception as e:
            self.logger.error(f"Error initializing model '{model_id}': {e}")
            raise

        # Prepare model settings
        model_settings = None
        # Responses API with built-in tools
        if provider == 'openai_responses' and openai_builtin_tools is not None:
            model_settings = OpenAIResponsesModelSettings(
                openai_builtin_tools=openai_builtin_tools,
                **({'max_tokens': tokens} if tokens is not None else {}),
            )
        # Other providers: respect max_tokens
        elif tokens is not None:
            model_settings = {'max_tokens': tokens}

        # Log info and debug
        self.logger.info(f"LLM call: provider={provider}, model={name}")
        self.logger.debug(
            f"LLM request payload: prompt={prompt!r}, max_tokens={tokens}, output_type={output_type}, mcp_servers={servers}, builtin_tools={openai_builtin_tools}"
        )

        # Create agent
        agent_kwargs: Dict[str, Any] = {
            'model': llm_model,
            'output_type': output_type,
            'mcp_servers': servers,
        }
        if model_settings is not None:
            agent_kwargs['model_settings'] = model_settings
        agent: Agent = Agent(**agent_kwargs)

        # Execute
        try:
            async with agent.run_mcp_servers():
                result = await agent.run(prompt)
        except Exception as e:
            self.logger.error(f"LLM call error for model '{model_id}': {e}", exc_info=True)
            raise

        # Log result and usage
        self.logger.debug(f"LLM raw result: {result!r}")
        try:
            usage = result.usage()
            self.logger.info(
                f"LLM completed: requests={usage.requests}, "
                f"request_tokens={usage.request_tokens}, response_tokens={usage.response_tokens}, "
                f"total_tokens={usage.total_tokens}"
            )
        except Exception:
            # usage not available
            pass

        return result.output
