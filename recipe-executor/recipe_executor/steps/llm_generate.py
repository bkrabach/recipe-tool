# This file was generated by Codebase-Generator, do not edit directly
import logging
from typing import Any, Dict, List, Optional, Union, Type

from pydantic import BaseModel

from recipe_executor.models import FileSpec
from recipe_executor.protocols import ContextProtocol
from recipe_executor.steps.base import BaseStep, StepConfig
from recipe_executor.utils.models import json_object_to_pydantic_model
from recipe_executor.utils.templates import render_template
from recipe_executor.llm_utils.mcp import get_mcp_server
from recipe_executor.llm_utils.llm import LLM


class LLMGenerateConfig(StepConfig):
    """
    Config for LLMGenerateStep.

    Fields:
        prompt: The prompt to send to the LLM (templated beforehand).
        model: The model identifier to use (provider/model_name format).
        max_tokens: The maximum number of tokens for the LLM response.
        mcp_servers: List of MCP servers for access to tools.
        openai_builtin_tools: Built-in tools for Responses API models.
        output_format: The format of the LLM output (text, files, or JSON schema object/list).
        output_key: The name under which to store the LLM output in context.
    """
    prompt: str
    model: str = "openai/gpt-4o"
    max_tokens: Optional[Union[str, int]] = None
    mcp_servers: Optional[List[Dict[str, Any]]] = None
    openai_builtin_tools: Optional[List[Dict[str, Any]]] = None
    output_format: Union[str, Dict[str, Any], List[Dict[str, Any]]]
    output_key: str = "llm_output"


class LLMGenerateStep(BaseStep[LLMGenerateConfig]):
    def __init__(self, logger: logging.Logger, config: Dict[str, Any]) -> None:
        super().__init__(logger, LLMGenerateConfig(**config))

    async def execute(self, context: ContextProtocol) -> None:
        # Render dynamic values
        prompt = render_template(self.config.prompt, context)
        model_id = render_template(self.config.model, context)
        # Max tokens
        max_tokens_val: Optional[int] = None
        if self.config.max_tokens is not None:
            raw = self.config.max_tokens
            if isinstance(raw, str):
                rendered = render_template(raw, context)
                max_tokens_val = int(rendered)
            else:
                max_tokens_val = int(raw)
        # Output key (dynamic)
        output_key = render_template(self.config.output_key, context)
        # Prepare MCP servers
        mcp_servers: List[Any] = []
        if self.config.mcp_servers:
            def _render(val: Any) -> Any:
                if isinstance(val, str):
                    return render_template(val, context)
                if isinstance(val, dict):
                    return {k: _render(v) for k, v in val.items()}
                if isinstance(val, list):
                    return [_render(v) for v in val]
                return val

            for server_conf in self.config.mcp_servers:
                rendered_conf = _render(server_conf)
                server = get_mcp_server(logger=self.logger, config=rendered_conf)
                mcp_servers.append(server)
        # Instantiate LLM
        llm = LLM(
            logger=self.logger,
            model=model_id,
            max_tokens=max_tokens_val,
            mcp_servers=mcp_servers or None,
        )
        # Validate built-in tools
        validated_tools: Optional[List[Dict[str, Any]]] = None
        if self.config.openai_builtin_tools:
            provider = model_id.split("/")[0]
            if provider not in ("openai_responses", "azure_responses"):
                raise ValueError(
                    "Built-in tools only supported with Responses API models (openai_responses/* or azure_responses/*)"
                )
            for tool in self.config.openai_builtin_tools:
                ttype = tool.get("type")
                if ttype != "web_search_preview":
                    raise ValueError(f"Unsupported tool type: {ttype}. Supported: web_search_preview")
            validated_tools = self.config.openai_builtin_tools
        # Determine output type
        fmt = self.config.output_format
        if isinstance(fmt, str) and fmt == "text":
            output_type: Union[Type[Any], Type[BaseModel]] = str
        elif isinstance(fmt, str) and fmt == "files":
            class FileSpecCollection(BaseModel):  # type: ignore
                files: List[FileSpec]

            output_type = FileSpecCollection
        elif isinstance(fmt, dict):
            # object schema
            output_type = json_object_to_pydantic_model(fmt, model_name="LLMGenerateOutputModel")
        elif isinstance(fmt, list) and len(fmt) == 1:
            # list schema
            item_schema = fmt[0]
            object_schema = {
                "type": "object",
                "properties": {"items": {"type": "array", "items": item_schema}},
                "required": ["items"],
            }
            output_type = json_object_to_pydantic_model(object_schema, model_name="LLMGenerateListOutputModel")
        else:
            raise ValueError(f"Invalid output_format: {fmt}")
        # Call LLM
        self.logger.debug("Calling LLM generate")
        try:
            result = await llm.generate(
                prompt=prompt,
                output_type=output_type,
                openai_builtin_tools=validated_tools,
            )
        except Exception as e:
            self.logger.error("LLM generate failed: %s", e)
            raise
        # Store results
        if isinstance(fmt, str) and fmt == "text":
            context[output_key] = result  # type: ignore
        elif isinstance(fmt, str) and fmt == "files":
            # result is FileSpecCollection
            files_list = getattr(result, "files", [])  # type: ignore
            # convert to dicts
            context[output_key] = [f.model_dump() for f in files_list]
        elif isinstance(fmt, dict):
            # object model
            context[output_key] = result.model_dump()  # type: ignore
        else:
            # list model
            data = result.model_dump()  # type: ignore
            items = data.get("items", [])
            context[output_key] = items
