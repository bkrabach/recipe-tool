# This file was generated by Codebase-Generator, do not edit directly
"""
ParallelStep allows concurrent execution of multiple sub-steps (execute_recipe steps) with isolation.
"""

import asyncio
import logging
from collections.abc import Awaitable
from typing import Any, Dict, List, Optional

from recipe_executor.steps.base import BaseStep, StepConfig
from recipe_executor.steps.registry import STEP_REGISTRY
from recipe_executor.protocols import ContextProtocol, StepProtocol


class ParallelConfig(StepConfig):
    """Config for ParallelStep.

    Fields:
        substeps: List of sub-step definitions, each a dict with 'type' and 'config'.
        max_concurrency: Maximum number of substeps to run concurrently. 0 means unlimited.
        delay: Optional delay (in seconds) between launching each substep.
        timeout: Optional timeout (in seconds) for the entire parallel execution.
    """

    substeps: List[Dict[str, Any]]
    max_concurrency: int = 0
    delay: float = 0.0
    timeout: Optional[float] = None


class ParallelStep(BaseStep[ParallelConfig]):
    """Step to execute multiple sub-steps in parallel."""

    def __init__(self, logger: logging.Logger, config: Dict[str, Any]) -> None:
        validated = ParallelConfig.model_validate(config)
        super().__init__(logger, validated)

    async def execute(self, context: ContextProtocol) -> None:
        substeps: List[Dict[str, Any]] = self.config.substeps or []
        total: int = len(substeps)
        max_conc: int = self.config.max_concurrency
        delay: float = self.config.delay
        timeout: Optional[float] = self.config.timeout

        self.logger.debug("ParallelStep configuration: %s", self.config)
        self.logger.info(
            "Starting ParallelStep: %d substeps, max_concurrency=%d, delay=%.3f, timeout=%s",
            total,
            max_conc,
            delay,
            f"{timeout}s" if timeout is not None else "none",
        )

        if total == 0:
            self.logger.info("ParallelStep has no substeps to execute. Skipping.")
            return

        # Determine concurrency limit (0 or negative means unlimited)
        concurrency: int = total if max_conc <= 0 else min(max_conc, total)
        semaphore = asyncio.Semaphore(concurrency)

        # Track first failure
        failure: Dict[str, Optional[Any]] = {"exc": None, "idx": None}
        tasks: List[asyncio.Task] = []

        async def run_substep(idx: int, spec: Dict[str, Any]) -> None:
            sub_logger = self.logger.getChild(f"substep_{idx}")
            try:
                sub_logger.debug(
                    "Preparing substep %d: clone context; spec=%s",
                    idx,
                    spec,
                )
                sub_context = context.clone()

                step_type = spec.get("type")
                step_cfg = spec.get("config", {}) or {}
                if not step_type or step_type not in STEP_REGISTRY:
                    raise RuntimeError(f"Unknown step type '{step_type}' for substep {idx}")
                StepClass = STEP_REGISTRY[step_type]
                step_instance: StepProtocol = StepClass(sub_logger, step_cfg)

                sub_logger.info("Launching substep %d of type '%s'", idx, step_type)
                result = step_instance.execute(sub_context)
                if isinstance(result, Awaitable):  # type: ignore
                    await result  # type: ignore
                sub_logger.info("Substep %d completed successfully", idx)

            except Exception as exc:
                if failure["exc"] is None:
                    failure["exc"] = exc
                    failure["idx"] = idx
                sub_logger.error("Substep %d failed: %s", idx, exc, exc_info=True)
                raise

            finally:
                semaphore.release()

        # Launch substeps with concurrency control and optional delay
        for idx, spec in enumerate(substeps):
            if failure["exc"]:
                self.logger.debug("Fail-fast: aborting launch of remaining substeps at index %d", idx)
                break

            await semaphore.acquire()
            if delay > 0:
                await asyncio.sleep(delay)

            task = asyncio.create_task(run_substep(idx, spec))
            tasks.append(task)

        if not tasks:
            self.logger.info("No substeps were launched. Nothing to wait for.")
            return

        # Wait until first exception, all done, or timeout
        try:
            if timeout is not None:
                done, pending = await asyncio.wait(tasks, timeout=timeout, return_when=asyncio.FIRST_EXCEPTION)
            else:
                done, pending = await asyncio.wait(tasks, return_when=asyncio.FIRST_EXCEPTION)
        except Exception:
            done, pending = set(tasks), set()

        # Handle failure
        if failure["exc"] is not None:
            failed_idx = failure.get("idx")
            self.logger.error(
                "A substep failed at index %s; cancelling remaining %d tasks",
                failed_idx,
                len(pending),
            )
            for p in pending:
                p.cancel()
            await asyncio.gather(*pending, return_exceptions=True)
            raise RuntimeError(f"ParallelStep aborted due to failure in substep {failed_idx}") from failure["exc"]

        # Handle timeout
        if pending:
            # No failure, but still pending: timed out
            self.logger.error(
                "ParallelStep timed out after %.3f seconds; cancelling %d pending tasks",
                timeout,
                len(pending),
            )
            for p in pending:
                p.cancel()
            await asyncio.gather(*pending, return_exceptions=True)
            raise asyncio.TimeoutError(f"ParallelStep timed out after {timeout} seconds")

        # All succeeded
        # Ensure propagation of any unexpected exceptions
        await asyncio.gather(*done)
        success_count = len(done)
        self.logger.info(
            "Completed ParallelStep: %d/%d substeps succeeded",
            success_count,
            total,
        )
